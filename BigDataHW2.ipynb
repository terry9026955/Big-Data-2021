{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe2bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d037c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+-------+-------------------+------------------+-------------------+--------+----------+--------+\n",
      "| IDLink|               Title|            Headline|              Source|  Topic|        PublishDate|    SentimentTitle|  SentimentHeadline|Facebook|GooglePlus|LinkedIn|\n",
      "+-------+--------------------+--------------------+--------------------+-------+-------------------+------------------+-------------------+--------+----------+--------+\n",
      "|99248.0|Obama Lays Wreath...|Obama Lays Wreath...|           USA TODAY|  obama|2002-04-02 00:00:00|                 0|-0.0533001790889026|      -1|        -1|      -1|\n",
      "|10423.0|A Look at the Hea...|Tim Haywood, inve...|           Bloomberg|economy|2008-09-20 00:00:00| 0.208333333333333| -0.156385810542806|      -1|        -1|      -1|\n",
      "|18828.0|Nouriel Roubini: ...|Nouriel Roubini, ...|           Bloomberg|economy|2012-01-28 00:00:00|-0.425210032135381|  0.139754248593737|      -1|        -1|      -1|\n",
      "|27788.0|Finland GDP Expan...|Finland's economy...|            RTT News|economy|2015-03-01 00:06:00|                 0| 0.0260643017571343|      -1|        -1|      -1|\n",
      "|27789.0|Tourism, govt spe...|Tourism and publi...|The Nation - Thai...|economy|2015-03-01 00:11:00|                 0|  0.141084456488315|      -1|        -1|      -1|\n",
      "+-------+--------------------+--------------------+--------------------+-------+-------------------+------------------+-------------------+--------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|               Title|            Headline|\n",
      "+--------------------+--------------------+\n",
      "|Obama Lays Wreath...|Obama Lays Wreath...|\n",
      "|A Look at the Hea...|Tim Haywood, inve...|\n",
      "|Nouriel Roubini: ...|Nouriel Roubini, ...|\n",
      "|Finland GDP Expan...|Finland's economy...|\n",
      "|Tourism, govt spe...|Tourism and publi...|\n",
      "|Intellitec Soluti...|Over 100 attendee...|\n",
      "| Monday, 29 Feb 2016|RAMALLAH, Februar...|\n",
      "|Obama, stars pay ...|First lady Michel...|\n",
      "|Fire claims more ...|A Hancock County ...|\n",
      "|Microsoft's new W...|New Delhi, Feb.29...|\n",
      "|Microsoft Project...|Microsoft may hav...|\n",
      "|Microsoft sneaks ...|The platform batt...|\n",
      "|Greek economy gro...|Greece's economy ...|\n",
      "|Big data and the ...|Big data analytic...|\n",
      "|HoloLens dev edit...|Microsoft’s AR he...|\n",
      "|Microsoft Word fo...|What is A + B? We...|\n",
      "|Microsoft Band 2 ...|The Microsoft Ban...|\n",
      "|Microsoft prepare...|It seems that Mic...|\n",
      "|Greek economy shr...|Greece's economy ...|\n",
      "|Sweden's economy ...|Sweden's economy ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#讀檔案\n",
    "\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "inferschema='true').load('C://Users//user//Desktop//BigDataHW2//News_Final.csv')\n",
    "data.show(5)\n",
    "\n",
    "#取Title跟Headline的dataframe\n",
    "data1 = data.select(\"Title\", \"Headline\")\n",
    "data1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d332893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               Title|count|\n",
      "+--------------------+-----+\n",
      "| Business Highlights|   37|\n",
      "|After Palestine t...|   23|\n",
      "|        ALSO IN News|   23|\n",
      "|5 Things to Know ...|   19|\n",
      "|Obama honors Conn...|   17|\n",
      "|Indonesia leader ...|   16|\n",
      "|Woman, 4 grandchi...|   15|\n",
      "|Microsoft's secre...|   14|\n",
      "|Obama praises Nan...|   13|\n",
      "|Brexit vote adds ...|   13|\n",
      "|The Latest: Flood...|   13|\n",
      "|Prospects still s...|   13|\n",
      "|Police identify P...|   12|\n",
      "|Windows 10 releas...|   12|\n",
      "|Obama wants $4B t...|   12|\n",
      "|Obama says North ...|   12|\n",
      "|Economy or first ...|   12|\n",
      "|With HoloLens, Mi...|   12|\n",
      "|Family of flood v...|   11|\n",
      "|Missing from the ...|   11|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            Headline|count|\n",
      "+--------------------+-----+\n",
      "|\"\"\"\"\"\"\"Microsoft ...|   21|\n",
      "|Read full story f...|   18|\n",
      "|                null|   15|\n",
      "|The views express...|   13|\n",
      "|President Obama o...|   11|\n",
      "|BETHLEHEM, March ...|   11|\n",
      "|The court is weig...|   11|\n",
      "|The family of a P...|   10|\n",
      "|In 2014, Palestin...|   10|\n",
      "|The Creighton Eco...|    9|\n",
      "|Police have ident...|    9|\n",
      "|Lisa Asberry Davi...|    9|\n",
      "|President Barack ...|    9|\n",
      "|With fresh fricti...|    8|\n",
      "|Boris Johnson's P...|    8|\n",
      "|RAMALLAH, March 3...|    8|\n",
      "|The subject was B...|    8|\n",
      "|President Obama a...|    8|\n",
      "|Ironically, Abbas...|    8|\n",
      "|Protecting his si...|    8|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#第一題\n",
    "\n",
    "#Title\n",
    "data1.groupBy(\"Title\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()\n",
    "\n",
    "#Headline\n",
    "data1.groupBy(\"Headline\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc07e28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Topic='obama', words='the', count=36647),\n",
       " Row(Topic='obama', words='to', count=23198),\n",
       " Row(Topic='obama', words='Obama', count=23075),\n",
       " Row(Topic='obama', words='President', count=17894),\n",
       " Row(Topic='obama', words='a', count=16574),\n",
       " Row(Topic='obama', words='of', count=16468),\n",
       " Row(Topic='obama', words='in', count=14016),\n",
       " Row(Topic='obama', words='Barack', count=12489),\n",
       " Row(Topic='obama', words='and', count=12358),\n",
       " Row(Topic='obama', words='on', count=10365),\n",
       " Row(Topic='obama', words='', count=7230),\n",
       " Row(Topic='obama', words='that', count=7082),\n",
       " Row(Topic='obama', words='his', count=7046),\n",
       " Row(Topic='obama', words='for', count=6924),\n",
       " Row(Topic='obama', words='is', count=5968),\n",
       " Row(Topic='obama', words='with', count=4956),\n",
       " Row(Topic='obama', words='at', count=4556),\n",
       " Row(Topic='obama', words='he', count=4222),\n",
       " Row(Topic='obama', words='The', count=4159),\n",
       " Row(Topic='obama', words='has', count=3924)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第一題之較正確的寫法\n",
    "from pyspark.sql.functions import explode, split, create_map, count, max, col\n",
    "\n",
    "#先把四個Topic分開\n",
    "df_oba = data.filter(data[\"Topic\"] == \"obama\")\n",
    "df_eco = data.filter(data[\"Topic\"] == \"economy\")\n",
    "df_mic = data.filter(data[\"Topic\"] == \"microsoft\")\n",
    "df_pal = data.filter(data[\"Topic\"] == \"palestine\")\n",
    "\n",
    "#obama topic\n",
    "df_oba.select(col(\"Topic\"), explode(split(df_oba.Title, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)\n",
    "df_oba.select(col(\"Topic\"), explode(split(df_oba.Headline, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "119bc92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Topic='economy', words='the', count=53488),\n",
       " Row(Topic='economy', words='of', count=22255),\n",
       " Row(Topic='economy', words='to', count=21123),\n",
       " Row(Topic='economy', words='in', count=20289),\n",
       " Row(Topic='economy', words='a', count=18441),\n",
       " Row(Topic='economy', words='economy', count=17477),\n",
       " Row(Topic='economy', words='and', count=16680),\n",
       " Row(Topic='economy', words='is', count=10199),\n",
       " Row(Topic='economy', words='The', count=9376),\n",
       " Row(Topic='economy', words='on', count=8475),\n",
       " Row(Topic='economy', words='that', count=7385),\n",
       " Row(Topic='economy', words='for', count=7290),\n",
       " Row(Topic='economy', words='', count=7188),\n",
       " Row(Topic='economy', words='economic', count=6896),\n",
       " Row(Topic='economy', words='as', count=5749),\n",
       " Row(Topic='economy', words='has', count=5155),\n",
       " Row(Topic='economy', words='by', count=4945),\n",
       " Row(Topic='economy', words='at', count=4287),\n",
       " Row(Topic='economy', words='with', count=3833),\n",
       " Row(Topic='economy', words='from', count=3781)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#economy topic\n",
    "df_eco.select(col(\"Topic\"), explode(split(df_eco.Title, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)\n",
    "df_eco.select(col(\"Topic\"), explode(split(df_eco.Headline, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6c51f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Topic='microsoft', words='the', count=25306),\n",
       " Row(Topic='microsoft', words='Microsoft', count=18925),\n",
       " Row(Topic='microsoft', words='to', count=16881),\n",
       " Row(Topic='microsoft', words='a', count=14077),\n",
       " Row(Topic='microsoft', words='of', count=13308),\n",
       " Row(Topic='microsoft', words='and', count=12105),\n",
       " Row(Topic='microsoft', words='in', count=8599),\n",
       " Row(Topic='microsoft', words='for', count=7694),\n",
       " Row(Topic='microsoft', words='', count=7384),\n",
       " Row(Topic='microsoft', words='is', count=7369),\n",
       " Row(Topic='microsoft', words='that', count=6304),\n",
       " Row(Topic='microsoft', words='its', count=6126),\n",
       " Row(Topic='microsoft', words='on', count=6041),\n",
       " Row(Topic='microsoft', words='has', count=5891),\n",
       " Row(Topic='microsoft', words='with', count=4830),\n",
       " Row(Topic='microsoft', words='Windows', count=4507),\n",
       " Row(Topic='microsoft', words='The', count=3978),\n",
       " Row(Topic='microsoft', words='it', count=3903),\n",
       " Row(Topic='microsoft', words='new', count=3500),\n",
       " Row(Topic='microsoft', words=\"Microsoft's\", count=3205)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#microsoft topic\n",
    "df_mic.select(col(\"Topic\"), explode(split(df_mic.Title, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)\n",
    "df_mic.select(col(\"Topic\"), explode(split(df_mic.Headline, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5efd887a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Topic='palestine', words='the', count=11566),\n",
       " Row(Topic='palestine', words='of', count=6807),\n",
       " Row(Topic='palestine', words='to', count=5153),\n",
       " Row(Topic='palestine', words='a', count=5092),\n",
       " Row(Topic='palestine', words='in', count=4994),\n",
       " Row(Topic='palestine', words='and', count=4264),\n",
       " Row(Topic='palestine', words='Palestinian', count=3230),\n",
       " Row(Topic='palestine', words='Palestine', count=3154),\n",
       " Row(Topic='palestine', words='on', count=2449),\n",
       " Row(Topic='palestine', words='', count=2319),\n",
       " Row(Topic='palestine', words='for', count=2189),\n",
       " Row(Topic='palestine', words='The', count=2028),\n",
       " Row(Topic='palestine', words='is', count=1566),\n",
       " Row(Topic='palestine', words='that', count=1451),\n",
       " Row(Topic='palestine', words='has', count=1288),\n",
       " Row(Topic='palestine', words='with', count=1211),\n",
       " Row(Topic='palestine', words='at', count=1202),\n",
       " Row(Topic='palestine', words='by', count=1172),\n",
       " Row(Topic='palestine', words='Israeli', count=1115),\n",
       " Row(Topic='palestine', words='was', count=1025)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#palestine topic\n",
    "df_pal.select(col(\"Topic\"), explode(split(df_pal.Title, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)\n",
    "df_pal.select(col(\"Topic\"), explode(split(df_pal.Headline, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7447b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------------------+----------------------+----------------------+\n",
      "|Topic|sum(SentimentTitle)| avg(SentimentTitle)|sum(SentimentHeadline)|avg(SentimentHeadline)|\n",
      "+-----+-------------------+--------------------+----------------------+----------------------+\n",
      "|obama|-15.743686315455108|-5.81484259111915...|    -481.8388358106591|  -0.01779644822938...|\n",
      "+-----+-------------------+--------------------+----------------------+----------------------+\n",
      "\n",
      "+-------+-------------------+--------------------+----------------------+----------------------+\n",
      "|  Topic|sum(SentimentTitle)| avg(SentimentTitle)|sum(SentimentHeadline)|avg(SentimentHeadline)|\n",
      "+-------+-------------------+--------------------+----------------------+----------------------+\n",
      "|economy|  -336.937004437336|-0.01047591967283...|   -1271.3909442082143|  -0.03952961304008377|\n",
      "+-------+-------------------+--------------------+----------------------+----------------------+\n",
      "\n",
      "+---------+-------------------+--------------------+----------------------+----------------------+\n",
      "|    Topic|sum(SentimentTitle)| avg(SentimentTitle)|sum(SentimentHeadline)|avg(SentimentHeadline)|\n",
      "+---------+-------------------+--------------------+----------------------+----------------------+\n",
      "|microsoft|  49.43849052234941|0.002310425765134...|   -318.81900083681893|  -0.01489947662570...|\n",
      "+---------+-------------------+--------------------+----------------------+----------------------+\n",
      "\n",
      "+---------+-------------------+--------------------+----------------------+----------------------+\n",
      "|    Topic|sum(SentimentTitle)| avg(SentimentTitle)|sum(SentimentHeadline)|avg(SentimentHeadline)|\n",
      "+---------+-------------------+--------------------+----------------------+----------------------+\n",
      "|palestine|-164.48440896913763|-0.01986526678371...|   -363.16995277671003|  -0.04386110540781...|\n",
      "+---------+-------------------+--------------------+----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#第三題\n",
    "\n",
    "data2 = data.select(\"Topic\", \"SentimentTitle\", \"SentimentHeadline\")\n",
    "\n",
    "\n",
    "#obama\n",
    "sentiment_score_oba = data2.filter(data2[\"Topic\"] == \"obama\").select(\"Topic\", \"SentimentTitle\",\"SentimentHeadline\")\n",
    "sentiment_score_oba.groupby(\"Topic\").agg(sum(\"SentimentTitle\"), mean(\"SentimentTitle\"), sum(\"SentimentHeadline\"), mean(\"SentimentHeadline\")).show()\n",
    "\n",
    "#economy\n",
    "sentiment_score_eco = data2.filter(data2[\"Topic\"] == \"economy\").select(\"Topic\", \"SentimentTitle\",\"SentimentHeadline\")\n",
    "sentiment_score_eco.groupby(\"Topic\").agg(sum(\"SentimentTitle\"), mean(\"SentimentTitle\"), sum(\"SentimentHeadline\"), mean(\"SentimentHeadline\")).show()\n",
    "\n",
    "#microsoft\n",
    "sentiment_score_mic = data2.filter(data2[\"Topic\"] == \"microsoft\").select(\"Topic\", \"SentimentTitle\",\"SentimentHeadline\")\n",
    "sentiment_score_mic.groupby(\"Topic\").agg(sum(\"SentimentTitle\"), mean(\"SentimentTitle\"), sum(\"SentimentHeadline\"), mean(\"SentimentHeadline\")).show()\n",
    "\n",
    "#palestine\n",
    "sentiment_score_pal = data2.filter(data2[\"Topic\"] == \"palestine\").select(\"Topic\", \"SentimentTitle\",\"SentimentHeadline\")\n",
    "sentiment_score_pal.groupby(\"Topic\").agg(sum(\"SentimentTitle\"), mean(\"SentimentTitle\"), sum(\"SentimentHeadline\"), mean(\"SentimentHeadline\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0095ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o45.load.\n: java.io.IOException: Incomplete HDFS URI, no host: hdfs:/Facebook_Economy.csv\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:170)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-085cdee38264>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mplatform\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopics_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mplatform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mplatforms_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'com.databricks.spark.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'true'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'true'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplatform_df\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mplatform_df\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdfs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-085cdee38264>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mplatform\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopics_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mplatform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mplatforms_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'com.databricks.spark.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'true'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'true'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplatform_df\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mplatform_df\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdfs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-085cdee38264>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mplatform\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopics_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mplatform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mplatforms_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'com.databricks.spark.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'true'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'true'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplatform_df\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mplatform_df\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdfs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o45.load.\n: java.io.IOException: Incomplete HDFS URI, no host: hdfs:/Facebook_Economy.csv\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:170)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "#第二題 不會寫\n",
    "\n",
    "data_dir = \"hdfs:/\"\n",
    "\n",
    "topics_name = [\"Economy\", \"Microsoft\", \"Obama\", \"Palestine\"]\n",
    "platforms_name = [\"Facebook\", \"GooglePlus\", \"LinkedIn\"]\n",
    "paths = [list(map(lambda x: data_dir + platform + \"_\" + x + \".csv\", topics_name)) for platform in platforms_name]\n",
    "\n",
    "dfs = [list(map(lambda x: spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(x), path)) for path in paths]\n",
    "\n",
    "dfs = [reduce(DataFrame.union, platform_df) for platform_df in dfs]\n",
    "\n",
    "k = 60 // 20\n",
    "dfs_hours = [df.select(col(\"IDLink\"), *[(reduce(lambda x, y: x + y, [col(df.columns[j]) for j in range(1+k*i, 1+k*i+k)])/k).alias(f\"hour{i}\") for i in range(0, len(df.columns)//k)]) for df in dfs]\n",
    "\n",
    "k = 24 * 60 // 20\n",
    "dfs_days = [df.select(col(\"IDLink\"), *[(reduce(lambda x, y: x + y, [col(df.columns[j]) for j in range(1+k*i, 1+k*i+k)])/k).alias(f\"day{i}\") for i in range(0, len(df.columns)//k)]) for df in dfs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce87a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第四題 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
