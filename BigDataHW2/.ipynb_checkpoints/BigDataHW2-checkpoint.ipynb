{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe2bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d037c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+-------+-------------------+------------------+-------------------+--------+----------+--------+\n",
      "| IDLink|               Title|            Headline|              Source|  Topic|        PublishDate|    SentimentTitle|  SentimentHeadline|Facebook|GooglePlus|LinkedIn|\n",
      "+-------+--------------------+--------------------+--------------------+-------+-------------------+------------------+-------------------+--------+----------+--------+\n",
      "|99248.0|Obama Lays Wreath...|Obama Lays Wreath...|           USA TODAY|  obama|2002-04-02 00:00:00|                 0|-0.0533001790889026|      -1|        -1|      -1|\n",
      "|10423.0|A Look at the Hea...|Tim Haywood, inve...|           Bloomberg|economy|2008-09-20 00:00:00| 0.208333333333333| -0.156385810542806|      -1|        -1|      -1|\n",
      "|18828.0|Nouriel Roubini: ...|Nouriel Roubini, ...|           Bloomberg|economy|2012-01-28 00:00:00|-0.425210032135381|  0.139754248593737|      -1|        -1|      -1|\n",
      "|27788.0|Finland GDP Expan...|Finland's economy...|            RTT News|economy|2015-03-01 00:06:00|                 0| 0.0260643017571343|      -1|        -1|      -1|\n",
      "|27789.0|Tourism, govt spe...|Tourism and publi...|The Nation - Thai...|economy|2015-03-01 00:11:00|                 0|  0.141084456488315|      -1|        -1|      -1|\n",
      "+-------+--------------------+--------------------+--------------------+-------+-------------------+------------------+-------------------+--------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|               Title|            Headline|\n",
      "+--------------------+--------------------+\n",
      "|Obama Lays Wreath...|Obama Lays Wreath...|\n",
      "|A Look at the Hea...|Tim Haywood, inve...|\n",
      "|Nouriel Roubini: ...|Nouriel Roubini, ...|\n",
      "|Finland GDP Expan...|Finland's economy...|\n",
      "|Tourism, govt spe...|Tourism and publi...|\n",
      "|Intellitec Soluti...|Over 100 attendee...|\n",
      "| Monday, 29 Feb 2016|RAMALLAH, Februar...|\n",
      "|Obama, stars pay ...|First lady Michel...|\n",
      "|Fire claims more ...|A Hancock County ...|\n",
      "|Microsoft's new W...|New Delhi, Feb.29...|\n",
      "|Microsoft Project...|Microsoft may hav...|\n",
      "|Microsoft sneaks ...|The platform batt...|\n",
      "|Greek economy gro...|Greece's economy ...|\n",
      "|Big data and the ...|Big data analytic...|\n",
      "|HoloLens dev edit...|Microsoft’s AR he...|\n",
      "|Microsoft Word fo...|What is A + B? We...|\n",
      "|Microsoft Band 2 ...|The Microsoft Ban...|\n",
      "|Microsoft prepare...|It seems that Mic...|\n",
      "|Greek economy shr...|Greece's economy ...|\n",
      "|Sweden's economy ...|Sweden's economy ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#讀檔案\n",
    "\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "inferschema='true').load('C://Users//user//Desktop//BigDataHW2//News_Final.csv')\n",
    "data.show(5)\n",
    "\n",
    "#取Title跟Headline的dataframe\n",
    "data1 = data.select(\"Title\", \"Headline\")\n",
    "data1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d332893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               Title|count|\n",
      "+--------------------+-----+\n",
      "| Business Highlights|   37|\n",
      "|After Palestine t...|   23|\n",
      "|        ALSO IN News|   23|\n",
      "|5 Things to Know ...|   19|\n",
      "|Obama honors Conn...|   17|\n",
      "|Indonesia leader ...|   16|\n",
      "|Woman, 4 grandchi...|   15|\n",
      "|Microsoft's secre...|   14|\n",
      "|Obama praises Nan...|   13|\n",
      "|Brexit vote adds ...|   13|\n",
      "|The Latest: Flood...|   13|\n",
      "|Prospects still s...|   13|\n",
      "|Police identify P...|   12|\n",
      "|Windows 10 releas...|   12|\n",
      "|Obama wants $4B t...|   12|\n",
      "|Obama says North ...|   12|\n",
      "|Economy or first ...|   12|\n",
      "|With HoloLens, Mi...|   12|\n",
      "|Family of flood v...|   11|\n",
      "|Missing from the ...|   11|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            Headline|count|\n",
      "+--------------------+-----+\n",
      "|\"\"\"\"\"\"\"Microsoft ...|   21|\n",
      "|Read full story f...|   18|\n",
      "|                null|   15|\n",
      "|The views express...|   13|\n",
      "|President Obama o...|   11|\n",
      "|BETHLEHEM, March ...|   11|\n",
      "|The court is weig...|   11|\n",
      "|The family of a P...|   10|\n",
      "|In 2014, Palestin...|   10|\n",
      "|The Creighton Eco...|    9|\n",
      "|Police have ident...|    9|\n",
      "|Lisa Asberry Davi...|    9|\n",
      "|President Barack ...|    9|\n",
      "|With fresh fricti...|    8|\n",
      "|Boris Johnson's P...|    8|\n",
      "|RAMALLAH, March 3...|    8|\n",
      "|The subject was B...|    8|\n",
      "|President Obama a...|    8|\n",
      "|Ironically, Abbas...|    8|\n",
      "|Protecting his si...|    8|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#第一題之不正確寫法\n",
    "\n",
    "#Title\n",
    "data1.groupBy(\"Title\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()\n",
    "\n",
    "#Headline\n",
    "data1.groupBy(\"Headline\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc07e28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Topic='obama', words='the', count=36647),\n",
       " Row(Topic='obama', words='to', count=23198),\n",
       " Row(Topic='obama', words='Obama', count=23075),\n",
       " Row(Topic='obama', words='President', count=17894),\n",
       " Row(Topic='obama', words='a', count=16574),\n",
       " Row(Topic='obama', words='of', count=16468),\n",
       " Row(Topic='obama', words='in', count=14016),\n",
       " Row(Topic='obama', words='Barack', count=12489),\n",
       " Row(Topic='obama', words='and', count=12358),\n",
       " Row(Topic='obama', words='on', count=10365),\n",
       " Row(Topic='obama', words='', count=7230),\n",
       " Row(Topic='obama', words='that', count=7082),\n",
       " Row(Topic='obama', words='his', count=7046),\n",
       " Row(Topic='obama', words='for', count=6924),\n",
       " Row(Topic='obama', words='is', count=5968),\n",
       " Row(Topic='obama', words='with', count=4956),\n",
       " Row(Topic='obama', words='at', count=4556),\n",
       " Row(Topic='obama', words='he', count=4222),\n",
       " Row(Topic='obama', words='The', count=4159),\n",
       " Row(Topic='obama', words='has', count=3924)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第一題之較正確的寫法\n",
    "from pyspark.sql.functions import explode, split, create_map, count, max, col\n",
    "\n",
    "#先把四個Topic分開\n",
    "df_oba = data.filter(data[\"Topic\"] == \"obama\")\n",
    "df_eco = data.filter(data[\"Topic\"] == \"economy\")\n",
    "df_mic = data.filter(data[\"Topic\"] == \"microsoft\")\n",
    "df_pal = data.filter(data[\"Topic\"] == \"palestine\")\n",
    "\n",
    "#obama topic\n",
    "df_oba.select(col(\"Topic\"), explode(split(df_oba.Title, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)\n",
    "df_oba.select(col(\"Topic\"), explode(split(df_oba.Headline, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "119bc92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Topic='economy', words='the', count=53488),\n",
       " Row(Topic='economy', words='of', count=22255),\n",
       " Row(Topic='economy', words='to', count=21123),\n",
       " Row(Topic='economy', words='in', count=20289),\n",
       " Row(Topic='economy', words='a', count=18441),\n",
       " Row(Topic='economy', words='economy', count=17477),\n",
       " Row(Topic='economy', words='and', count=16680),\n",
       " Row(Topic='economy', words='is', count=10199),\n",
       " Row(Topic='economy', words='The', count=9376),\n",
       " Row(Topic='economy', words='on', count=8475),\n",
       " Row(Topic='economy', words='that', count=7385),\n",
       " Row(Topic='economy', words='for', count=7290),\n",
       " Row(Topic='economy', words='', count=7188),\n",
       " Row(Topic='economy', words='economic', count=6896),\n",
       " Row(Topic='economy', words='as', count=5749),\n",
       " Row(Topic='economy', words='has', count=5155),\n",
       " Row(Topic='economy', words='by', count=4945),\n",
       " Row(Topic='economy', words='at', count=4287),\n",
       " Row(Topic='economy', words='with', count=3833),\n",
       " Row(Topic='economy', words='from', count=3781)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#economy topic\n",
    "df_eco.select(col(\"Topic\"), explode(split(df_eco.Title, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)\n",
    "df_eco.select(col(\"Topic\"), explode(split(df_eco.Headline, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6c51f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Topic='microsoft', words='the', count=25306),\n",
       " Row(Topic='microsoft', words='Microsoft', count=18925),\n",
       " Row(Topic='microsoft', words='to', count=16881),\n",
       " Row(Topic='microsoft', words='a', count=14077),\n",
       " Row(Topic='microsoft', words='of', count=13308),\n",
       " Row(Topic='microsoft', words='and', count=12105),\n",
       " Row(Topic='microsoft', words='in', count=8599),\n",
       " Row(Topic='microsoft', words='for', count=7694),\n",
       " Row(Topic='microsoft', words='', count=7384),\n",
       " Row(Topic='microsoft', words='is', count=7369),\n",
       " Row(Topic='microsoft', words='that', count=6304),\n",
       " Row(Topic='microsoft', words='its', count=6126),\n",
       " Row(Topic='microsoft', words='on', count=6041),\n",
       " Row(Topic='microsoft', words='has', count=5891),\n",
       " Row(Topic='microsoft', words='with', count=4830),\n",
       " Row(Topic='microsoft', words='Windows', count=4507),\n",
       " Row(Topic='microsoft', words='The', count=3978),\n",
       " Row(Topic='microsoft', words='it', count=3903),\n",
       " Row(Topic='microsoft', words='new', count=3500),\n",
       " Row(Topic='microsoft', words=\"Microsoft's\", count=3205)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#microsoft topic\n",
    "df_mic.select(col(\"Topic\"), explode(split(df_mic.Title, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)\n",
    "df_mic.select(col(\"Topic\"), explode(split(df_mic.Headline, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5efd887a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Topic='palestine', words='the', count=11566),\n",
       " Row(Topic='palestine', words='of', count=6807),\n",
       " Row(Topic='palestine', words='to', count=5153),\n",
       " Row(Topic='palestine', words='a', count=5092),\n",
       " Row(Topic='palestine', words='in', count=4994),\n",
       " Row(Topic='palestine', words='and', count=4264),\n",
       " Row(Topic='palestine', words='Palestinian', count=3230),\n",
       " Row(Topic='palestine', words='Palestine', count=3154),\n",
       " Row(Topic='palestine', words='on', count=2449),\n",
       " Row(Topic='palestine', words='', count=2319),\n",
       " Row(Topic='palestine', words='for', count=2189),\n",
       " Row(Topic='palestine', words='The', count=2028),\n",
       " Row(Topic='palestine', words='is', count=1566),\n",
       " Row(Topic='palestine', words='that', count=1451),\n",
       " Row(Topic='palestine', words='has', count=1288),\n",
       " Row(Topic='palestine', words='with', count=1211),\n",
       " Row(Topic='palestine', words='at', count=1202),\n",
       " Row(Topic='palestine', words='by', count=1172),\n",
       " Row(Topic='palestine', words='Israeli', count=1115),\n",
       " Row(Topic='palestine', words='was', count=1025)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#palestine topic\n",
    "df_pal.select(col(\"Topic\"), explode(split(df_pal.Title, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)\n",
    "df_pal.select(col(\"Topic\"), explode(split(df_pal.Headline, \" \")).alias('words')).groupBy(\"Topic\", \"words\").count().orderBy(desc(\"Topic\"), desc(\"count\")).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac0095ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第二題 \n",
    "from functools import reduce\n",
    "\n",
    "data_dir = \"C://Users//user//Desktop//BigDataHW2//\"\n",
    "\n",
    "topics_name = [\"Economy\", \"Microsoft\", \"Obama\", \"Palestine\"]\n",
    "platforms_name = [\"Facebook\", \"GooglePlus\", \"LinkedIn\"]\n",
    "paths = [list(map(lambda x: data_dir + platform + \"_\" + x + \".csv\", topics_name)) for platform in platforms_name]\n",
    "\n",
    "dfs = [list(map(lambda x: spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(x), path)) for path in paths]\n",
    "\n",
    "dfs = [reduce(DataFrame.union, platform_df) for platform_df in dfs]\n",
    "\n",
    "k = 60 // 20\n",
    "dfs_hours = [df.select(col(\"IDLink\"), *[(reduce(lambda x, y: x + y, [col(df.columns[j]) for j in range(1+k*i, 1+k*i+k)])/k).alias(f\"hour{i}\") for i in range(0, len(df.columns)//k)]) for df in dfs]\n",
    "\n",
    "k = 24 * 60 // 20\n",
    "dfs_days = [df.select(col(\"IDLink\"), *[(reduce(lambda x, y: x + y, [col(df.columns[j]) for j in range(1+k*i, 1+k*i+k)])/k).alias(f\"day{i}\") for i in range(0, len(df.columns)//k)]) for df in dfs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7447b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------------------+----------------------+----------------------+\n",
      "|Topic|sum(SentimentTitle)| avg(SentimentTitle)|sum(SentimentHeadline)|avg(SentimentHeadline)|\n",
      "+-----+-------------------+--------------------+----------------------+----------------------+\n",
      "|obama|-15.743686315455108|-5.81484259111915...|    -481.8388358106591|  -0.01779644822938...|\n",
      "+-----+-------------------+--------------------+----------------------+----------------------+\n",
      "\n",
      "+-------+-------------------+--------------------+----------------------+----------------------+\n",
      "|  Topic|sum(SentimentTitle)| avg(SentimentTitle)|sum(SentimentHeadline)|avg(SentimentHeadline)|\n",
      "+-------+-------------------+--------------------+----------------------+----------------------+\n",
      "|economy|  -336.937004437336|-0.01047591967283...|   -1271.3909442082143|  -0.03952961304008377|\n",
      "+-------+-------------------+--------------------+----------------------+----------------------+\n",
      "\n",
      "+---------+-------------------+--------------------+----------------------+----------------------+\n",
      "|    Topic|sum(SentimentTitle)| avg(SentimentTitle)|sum(SentimentHeadline)|avg(SentimentHeadline)|\n",
      "+---------+-------------------+--------------------+----------------------+----------------------+\n",
      "|microsoft|  49.43849052234941|0.002310425765134...|   -318.81900083681893|  -0.01489947662570...|\n",
      "+---------+-------------------+--------------------+----------------------+----------------------+\n",
      "\n",
      "+---------+-------------------+--------------------+----------------------+----------------------+\n",
      "|    Topic|sum(SentimentTitle)| avg(SentimentTitle)|sum(SentimentHeadline)|avg(SentimentHeadline)|\n",
      "+---------+-------------------+--------------------+----------------------+----------------------+\n",
      "|palestine|-164.48440896913763|-0.01986526678371...|   -363.16995277671003|  -0.04386110540781...|\n",
      "+---------+-------------------+--------------------+----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#第三題\n",
    "\n",
    "data2 = data.select(\"Topic\", \"SentimentTitle\", \"SentimentHeadline\")\n",
    "\n",
    "\n",
    "#obama\n",
    "sentiment_score_oba = data2.filter(data2[\"Topic\"] == \"obama\").select(\"Topic\", \"SentimentTitle\",\"SentimentHeadline\")\n",
    "sentiment_score_oba.groupby(\"Topic\").agg(sum(\"SentimentTitle\"), mean(\"SentimentTitle\"), sum(\"SentimentHeadline\"), mean(\"SentimentHeadline\")).show()\n",
    "\n",
    "#economy\n",
    "sentiment_score_eco = data2.filter(data2[\"Topic\"] == \"economy\").select(\"Topic\", \"SentimentTitle\",\"SentimentHeadline\")\n",
    "sentiment_score_eco.groupby(\"Topic\").agg(sum(\"SentimentTitle\"), mean(\"SentimentTitle\"), sum(\"SentimentHeadline\"), mean(\"SentimentHeadline\")).show()\n",
    "\n",
    "#microsoft\n",
    "sentiment_score_mic = data2.filter(data2[\"Topic\"] == \"microsoft\").select(\"Topic\", \"SentimentTitle\",\"SentimentHeadline\")\n",
    "sentiment_score_mic.groupby(\"Topic\").agg(sum(\"SentimentTitle\"), mean(\"SentimentTitle\"), sum(\"SentimentHeadline\"), mean(\"SentimentHeadline\")).show()\n",
    "\n",
    "#palestine\n",
    "sentiment_score_pal = data2.filter(data2[\"Topic\"] == \"palestine\").select(\"Topic\", \"SentimentTitle\",\"SentimentHeadline\")\n",
    "sentiment_score_pal.groupby(\"Topic\").agg(sum(\"SentimentTitle\"), mean(\"SentimentTitle\"), sum(\"SentimentHeadline\"), mean(\"SentimentHeadline\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce87a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第四題 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
